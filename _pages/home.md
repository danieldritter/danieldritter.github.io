---
layout: single
permalink: /test
---

## About Me
I'm currently a research assistant in the Debora Marks Lab at Harvard Medical School, working on applying machine learning methods to problems in biological sequence modeling. Previouly, I received my master's in computer science from the University of Oxford, and before that studied computer science and political science at Brown University. I'm broadly interested in questions of interpretability and explainability in machine learning. This ranges from technical questions, such as how we generate different kinds of explanations, to more social scientific or public policy ones (how useful are different interpretability techniques in practice?, how do we incentivize the development of interpretable ML applications?, etc.).
## Projects
### Assessing the Interpretability of Large Language Models
For my master's dissertation, I evaluated the quality of different black-box interpretability methods when applied across a range of different large language models. This involved applying several previously published frameworks for assessing the quality of model interpretations to many new models and methods, extending their beyond the smaller models often studied in the initial publications of these frameworks. This project was supervised by Yarin Gal, with help from Lisa Schut, Andrew Jesson, and Been Kim. The full text of my dissertation is available [here](/assets/pdfs/msc_thesis.pdf)
### Multiagent Planning via Partial Coordination in Markov Games
This project was my senior honors thesis, supervised by Michael Littman and Mark Ho, and focused on finding methods for scaling multiagent planning algorithms in markov games. Learning and planning algorithms in markov games tend to scale very poorly as you increase the number of agents in the game. To avoid this, we proposed a 'model game' framework, in which each agent has a simplified internal model of the environment, and computes a plan using that model. A copy of my thesis can be found [here](/assets/pdfs/Thesis.pdf).
### DeepLTLf: Learning Finite Linear Temporal Logic Specifications with a Specialized Neural Operator
This project was focused on the problem of learning linear temporal logic formulae from example temporal traces. We devised a specialized neural network architecture to learn LTL operators that could scale well beyond previous SAT-based approaches and handle noisy data more robustly. A copy of the resulting paper can be found [here](https://arxiv.org/abs/2111.04147).
## Links
### Resume
### Github 
