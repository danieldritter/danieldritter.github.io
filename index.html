---
layout: single
author_profile: true
---

<h2>About Me</h2>
<p>
I'm currently a software engineer/RA in the Deborah Marks Lab at Harvard Medical School, working on applying machine 
learning methods to problems in biological sequence modeling. Previously, I received my master's in computer science 
from the University of Oxford, and before that studied computer science and political science at Brown University. 
I'm broadly interested in questions of safety and fairness in machine learning (ML), and particularly in how we can evaluate and interpret
the decisions of complex ML models. This ranges from technical 
questions, such as developing methods to ensure that ML model decisions are fair and comprehensible, to ones more in the domain of social science or public policy 
ones (e.g. how useful are different technical methods in practice?, how do we incentivize the development of 
safe/fair ML practices?, etc.).
</p>

<h2>Projects</h2>
<h3>Assessing the Interpretability of Large Language Models</h3>
<p>
For my master's dissertation, I evaluated the quality of different black-box interpretability methods when used 
across a range of different large language models. This involved applying several previously published frameworks for 
assessing the quality of model interpretations to many new models and methods, extending their results beyond the smaller models 
often studied in the initial publications of these frameworks. This project was supervised by Yarin Gal, with help from 
Lisa Schut, Andrew Jesson, and Been Kim. The full text of my dissertation is available <a href="/assets/pdfs/msc_thesis.pdf">here</a>.
</p>
<h3>Multiagent Planning via Partial Coordination in Markov Games</h3>
<p>
This project was my senior honors thesis, supervised by Michael Littman and Mark Ho, and focused on
finding methods for scaling multiagent planning algorithms in markov games. Learning and planning algorithms 
in markov games tend to scale very poorly as you increase the number of agents in the game. To avoid this, we 
proposed a 'model game' framework, in which each agent has a simplified internal model of the environment, and 
computes a plan using that model. A pdf of my thesis can be found <a href=/assets/pdfs/thesis.pdf>here</a>.</p>
<h3>DeepLTLf: Learning Finite Linear Temporal Logic Specifications with a Specialized Neural Operator</h3>
<p>
This project was focused on the problem of learning linear temporal logic formulae from example temporal traces. 
We devised a specialized neural network architecture to learn LTL operators that could scale well beyond previous 
SAT-based approaches and handle noisy data more robustly. A copy of the resulting paper can be found 
<a href=https://arxiv.org/abs/2111.04147>here</a>.
</p>

<h2>Links</h2>
<ul>
    <li><a href="/assets/pdfs/resume.pdf">Resume</a></li>
    <li><a href="https://github.com/danieldritter">Github</a></li>
    </ul>
